name: KB Newspaper Scraper

on:
  # For manual triggering
  workflow_dispatch:
    inputs:
      year:
        description: 'Year to scrape (e.g., 1865)'
        required: true
        default: '1865'
      month:
        description: 'Month to scrape (1-12)'
        required: true
        default: '1'
      operation:
        description: 'Operation to perform'
        required: true
        default: 'start-month'
        type: choice
        options:
          - start-month
          - continue-scraping
          - retry-failed
          - verify-month
      newspaper_ids:
        description: 'Newspaper IDs (comma-separated, leave empty for default)'
        required: false
        default: ''
  
  # Uncomment this section when ready to set up a scheduled CRON job
  # schedule:
  #   - cron: '0 2 * * *'  # Run daily at 2 AM UTC

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
      
      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium webdriver-manager requests
      
      - name: Cache newspaper downloads
        uses: actions/cache@v2
        with:
          path: kb_newspapers
          key: kb-newspapers-${{ github.sha }}
          restore-keys: |
            kb-newspapers-
      
      - name: Cache scraper state
        uses: actions/cache@v2
        with:
          path: scraper_state.json
          key: scraper-state-${{ github.sha }}
          restore-keys: |
            scraper-state-
      
      - name: Start new month (if selected)
        if: ${{ github.event.inputs.operation == 'start-month' }}
        run: |
          python kb_newspaper_cron.py \
            --year ${{ github.event.inputs.year }} \
            --month ${{ github.event.inputs.month }} \
            --download-dir kb_newspapers \
            ${{ github.event.inputs.newspaper_ids != '' && format('--newspapers {0}', github.event.inputs.newspaper_ids) || '' }}
      
      - name: Continue scraping (if selected)
        if: ${{ github.event.inputs.operation == 'continue-scraping' }}
        run: |
          python kb_newspaper_cron.py \
            --download-dir kb_newspapers
      
      - name: Retry failed days (if selected)
        if: ${{ github.event.inputs.operation == 'retry-failed' }}
        run: |
          python kb_newspaper_cron.py \
            --retry-failed \
            --download-dir kb_newspapers
      
      - name: Verify month (if selected)
        if: ${{ github.event.inputs.operation == 'verify-month' }}
        run: |
          python kb_newspaper_cron.py \
            --verify-month \
            --download-dir kb_newspapers
      
      - name: Upload scraper logs
        uses: actions/upload-artifact@v2
        with:
          name: scraper-logs
          path: |
            kb_newspaper_cron.log
            kb_scraper.log
      
      - name: Upload scraper state
        uses: actions/upload-artifact@v2
        with:
          name: scraper-state
          path: scraper_state.json
      
      # Optional: upload downloaded newspapers as artifacts
      # This might be large - comment out if you don't want to upload
      - name: Upload downloaded newspapers
        uses: actions/upload-artifact@v2
        with:
          name: kb-newspapers
          path: kb_newspapers/
